# Herangehensweise: Wie genau soll das Ziel erreicht werden?

Um an die wichtigsten Aktienkennzahlen zu gelangen, mussten zunächst verschiedene Libaries und APIs für Finanzdaten untersucht und ihre Vor-und Nachteile herausgearbeitet werden. Die API von [Alpha Vantage](../python_files/alphavantage.py) erwies sich schnell als die beste Wahl, da sie die meisten Vorteile bot. Die API bietet umfangreiche Daten und ist kostenlos. Die Kennzahlen der Unternehmen und deren Aktien befinden im Abschnitt "Fundamental Data" und können mit dem vorgefertigten API-Request "Company Overview" aufgerufen werden. Der Request benötigt unter anderem ein Symbol oder Ticker als Parameter. Die Ticker können aus einer anderen Librarie wie bspw. der [yfinance Library](../notebooks/yahoo.ipynb) extrahiert und in einer [txt-Datei](../data/ticker.txt) gespeichert werden. Im nächsten Schritt wurde ein Algorithmus entwickelt, der alle Ticker aus der Datei einliest und durch diese in einer for-Loop iteriert. Diese Ticker konnten dann dem Request als Symbol-Parameter übergeben werden. Die Abfrage liefert als Ergebnis ein JSON-Objekt mit den  Unternehmensdaten, welche im letzten Schritt in einer [CSV-Datei](../data/stocks_data.csv) gespeichert werden können, um die Daten künftig auswerten zu können.

Ein weitere wichtiger Bestandteile des Projekts war der [Web Scraper](../notebooks/web_scraper.ipynb). Mit diesem sollen tagesaktuelle Nachrichten zu bestimmten Unternehmen und deren Branche extrahiert werden. Damit sollen später Investionsentscheidung schnell gefällt oder revidiert werden können. Hierfür wurden verschiedenen Nachrichten-& Finanzseiten untersucht, die zu jeder Branche die entsprechenden Unternehmensmeldungen übersichtlich darstellen. Bloomberg wäre hierfür die beste Wahl gewesen, allerdings werden die Artikel hinter eine Paywall verborgen. Eine Alternative für Bloomberg war die Seite [finanzen.net](https://finanzen.net) die für jede Branche wie z.B. Automobile & Verkehr oder Banken & Versicherungen alle Artikel übersichtlich in sogenannten News Cards abbildet. Eine News Card setzt sich aus einer Überschrift zusammen, die auch zugleich der Hyperlink zu dem Artikel ist und einem Teaser, der kurz zusammenfasst, worum es in dem Artikel geht. Das Ziel war es, aus allen News Cards die Hyperlinks zu extrahieren um diese in einem nächsten Schritt dazu zu verwenden, um den gesamten Inhalt des jeweiligen Artikel auszulesen und anschließend in einer JSON-Datei zwischen zu speichern. Für das Scraping als solches wurde die Bibliothek Beautiful Soup verwendet, mit der es ganz einfach war, die einzelnen HTML-Tags anzusteuern. Die wesentlichen Inhalte eines Artikel befanden sich zum Großteil in den Überschriften und div-Containern oder p-Tags.






